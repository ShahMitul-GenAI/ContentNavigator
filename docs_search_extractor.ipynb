{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dc4335-c03b-44d4-bec2-331d95b03d2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'langchain_practices-WLeYacI5 (Python 3.10.14)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/Mast_Nijanand/.virtualenvs/langchain_practices-WLeYacI5/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pathlib\n",
    "from os import listdir\n",
    "from dotenv import load_dotenv\n",
    "from os.path import isfile, join\n",
    "from IPython.display import display, Markdown\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_community.document_loaders.word_document import UnstructuredWordDocumentLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader, CSVLoader, JSONLoader, TextLoader, UnstructuredFileLoader, DirectoryLoader, \\\n",
    "    Docx2txtLoader, UnstructuredPowerPointLoader\n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bade09ca-887c-4305-9415-b07232cf0a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading API keys from env\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\n",
    "INDEX_NAME = os.environ.get(\"INDEX_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0adc50e9-415c-4973-91b2-3b3dc88a459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide file path & definingn functions  \n",
    "file_path = \"./docs/\"\n",
    "\n",
    "# Define a dictionary to map file extensions to their respective loaders\n",
    "loaders = {\n",
    "    '.pdf': PyPDFLoader,\n",
    "    '.doc': Docx2txtLoader,\n",
    "    '.docx': Docx2txtLoader,\n",
    "    '.csv': CSVLoader,\n",
    "    '.json': JSONLoader,\n",
    "    '.txt': TextLoader,\n",
    "    '.htm': UnstructuredFileLoader,\n",
    "    '.html': UnstructuredFileLoader,\n",
    "    '.ppt': UnstructuredPowerPointLoader,\n",
    "    '.pptx': UnstructuredPowerPointLoader,\n",
    "}\n",
    "\n",
    "# Get file type\n",
    "my_files = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n",
    "\n",
    "# Define a function to create a DirectoryLoader to load text from different formats\n",
    "# https://github.com/langchain-ai/langchain/discussions/18559\n",
    "# https://github.com/langchain-ai/langchain/discussions/9605\n",
    "\n",
    "def create_directory_loader(file_type):\n",
    "    if file_type == '.txt':\n",
    "        text_loader_kwargs={'autodetect_encoding': True}\n",
    "        return DirectoryLoader(\n",
    "            path=file_path,\n",
    "            glob=f\"**/*{file_type}\",\n",
    "            loader_cls=loaders[file_type],\n",
    "            silent_errors = True,\n",
    "            loader_kwargs=text_loader_kwargs,\n",
    "        )\n",
    "    else:\n",
    "        return DirectoryLoader(\n",
    "            path=file_path,\n",
    "            glob=f\"**/*{file_type}\",\n",
    "            loader_cls=loaders[file_type],\n",
    "            silent_errors = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd798cc2-a19b-4a4c-a49c-158c78fad20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gathering file contents\n",
    "\n",
    "documents = []\n",
    "for each in my_files:\n",
    "    file_extension = pathlib.Path(each).suffix\n",
    "    documents.extend(create_directory_loader(file_extension).load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6041ae3-d778-4dc4-9b93-4276f948945a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1318, which is longer than the specified 1000\n",
      "Created a chunk of size 1271, which is longer than the specified 1000\n",
      "Created a chunk of size 1156, which is longer than the specified 1000\n",
      "Created a chunk of size 1139, which is longer than the specified 1000\n",
      "Created a chunk of size 1171, which is longer than the specified 1000\n",
      "Created a chunk of size 1318, which is longer than the specified 1000\n",
      "Created a chunk of size 1271, which is longer than the specified 1000\n",
      "Created a chunk of size 1156, which is longer than the specified 1000\n",
      "Created a chunk of size 1139, which is longer than the specified 1000\n",
      "Created a chunk of size 1171, which is longer than the specified 1000\n",
      "Created a chunk of size 1175, which is longer than the specified 1000\n",
      "Created a chunk of size 1001, which is longer than the specified 1000\n",
      "Created a chunk of size 1163, which is longer than the specified 1000\n",
      "Created a chunk of size 1293, which is longer than the specified 1000\n",
      "Created a chunk of size 1400, which is longer than the specified 1000\n",
      "Created a chunk of size 1226, which is longer than the specified 1000\n",
      "Created a chunk of size 1098, which is longer than the specified 1000\n",
      "Created a chunk of size 1116, which is longer than the specified 1000\n",
      "Created a chunk of size 1610, which is longer than the specified 1000\n",
      "Created a chunk of size 1221, which is longer than the specified 1000\n",
      "Created a chunk of size 1030, which is longer than the specified 1000\n",
      "Created a chunk of size 1804, which is longer than the specified 1000\n",
      "Created a chunk of size 1554, which is longer than the specified 1000\n",
      "Created a chunk of size 1180, which is longer than the specified 1000\n",
      "Created a chunk of size 2366, which is longer than the specified 1000\n",
      "Created a chunk of size 1009, which is longer than the specified 1000\n",
      "Created a chunk of size 1175, which is longer than the specified 1000\n",
      "Created a chunk of size 1001, which is longer than the specified 1000\n",
      "Created a chunk of size 1163, which is longer than the specified 1000\n",
      "Created a chunk of size 1293, which is longer than the specified 1000\n",
      "Created a chunk of size 1400, which is longer than the specified 1000\n",
      "Created a chunk of size 1226, which is longer than the specified 1000\n",
      "Created a chunk of size 1098, which is longer than the specified 1000\n",
      "Created a chunk of size 1116, which is longer than the specified 1000\n",
      "Created a chunk of size 1610, which is longer than the specified 1000\n",
      "Created a chunk of size 1221, which is longer than the specified 1000\n",
      "Created a chunk of size 1030, which is longer than the specified 1000\n",
      "Created a chunk of size 1804, which is longer than the specified 1000\n",
      "Created a chunk of size 1554, which is longer than the specified 1000\n",
      "Created a chunk of size 1180, which is longer than the specified 1000\n",
      "Created a chunk of size 2366, which is longer than the specified 1000\n",
      "Created a chunk of size 1009, which is longer than the specified 1000\n",
      "Created a chunk of size 1009, which is longer than the specified 1000\n",
      "Created a chunk of size 1175, which is longer than the specified 1000\n",
      "Created a chunk of size 1001, which is longer than the specified 1000\n",
      "Created a chunk of size 1163, which is longer than the specified 1000\n",
      "Created a chunk of size 1293, which is longer than the specified 1000\n",
      "Created a chunk of size 1400, which is longer than the specified 1000\n",
      "Created a chunk of size 1226, which is longer than the specified 1000\n",
      "Created a chunk of size 1098, which is longer than the specified 1000\n",
      "Created a chunk of size 1116, which is longer than the specified 1000\n",
      "Created a chunk of size 1610, which is longer than the specified 1000\n",
      "Created a chunk of size 1221, which is longer than the specified 1000\n",
      "Created a chunk of size 1030, which is longer than the specified 1000\n",
      "Created a chunk of size 1804, which is longer than the specified 1000\n",
      "Created a chunk of size 1554, which is longer than the specified 1000\n",
      "Created a chunk of size 1180, which is longer than the specified 1000\n",
      "Created a chunk of size 2366, which is longer than the specified 1000\n",
      "Created a chunk of size 1009, which is longer than the specified 1000\n",
      "Created a chunk of size 1318, which is longer than the specified 1000\n",
      "Created a chunk of size 1271, which is longer than the specified 1000\n",
      "Created a chunk of size 1156, which is longer than the specified 1000\n",
      "Created a chunk of size 1139, which is longer than the specified 1000\n",
      "Created a chunk of size 1171, which is longer than the specified 1000\n",
      "Created a chunk of size 1009, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 1077 text chunks created.\n"
     ]
    }
   ],
   "source": [
    "# spliting documents text\n",
    "text_splitter = CharacterTextSplitter(chunk_size = 1000, chunk_overlap=50)\n",
    "split_content = text_splitter.split_documents(documents)\n",
    "print(f\"Total {len(split_content)} text chunks created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0915ed7-d202-44a4-84e6-10f209251c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clearning Pinecone index for repetitive useage\n",
    "def clear_vectorDB(inx):\n",
    "    index = pc.Index(inx)\n",
    "    index.delete(\n",
    "        delete.all == True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee03fa53-7aa0-4dc0-a34a-8d4f67005724",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mast_Nijanand\\anaconda3\\envs\\aiapp\\lib\\site-packages\\pydantic\\_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The content pool is small enough to be handled by the local database is used to handle the query\n"
     ]
    }
   ],
   "source": [
    "# storing the content In-Memory Vector Store \n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "try:\n",
    "    database = DocArrayInMemorySearch.from_documents(\n",
    "        documents,\n",
    "        embeddings,\n",
    "    )\n",
    "    print(\"The content pool is small enough to be handled by the local database is used to handle the query\")\n",
    "except:\n",
    "    print(\"The content pool is large enough to be handled by the local database. \\n\")\n",
    "    print(\"Pinecone Vector Database is now used for embedding & retrieval\")\n",
    "    \n",
    "    flrg = open(str(file_path) + \"LARGE.txt\", \"wb\")\n",
    "    pickle.dump(123, flrg)\n",
    "    \n",
    "    # getting Pinecone credentials from the server\n",
    "    pnc = []\n",
    "    pnc = pd.read_pickle(str(file_path) + \"pnc_vals.pkl\")\n",
    "\n",
    "    if len(pnc) > 0:\n",
    "        pc = Pinecone(api_key=os.environ.get(pnc[0]))\n",
    "        INDEX_NAME = pnc[1]\n",
    "    else:\n",
    "        pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
    "        INDEX_NAME = INDEX_NAME\n",
    "    \n",
    "    clear_vectorDB(INDEX_NAME)\n",
    "    database = PineConeVectorStore.from_documents(\n",
    "        documents = documents,\n",
    "        embedding = embeddings,\n",
    "        index_name = INDEX_NAME,\n",
    "    )\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6b0656b-a5a9-4867-8a12-333b5296a9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up retreival\n",
    "qa_ans = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type = 'stuff',\n",
    "    retriever = database.as_retriever(),\n",
    "    return_source_documents = True,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87e78154-a4f0-4a3e-9304-3794bb22020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Develop query for searching documents\n",
    "# file_path = os.path.dirname(os.path.abspath(__file__))\n",
    "with open(\"./docs/user_query.txt\", \"r\", encoding='utf-8') as f:\n",
    "    question = f.read()\n",
    "# question = \"What are Donald Trump's major achievements as the president?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a1bd057-b658-4731-b2b8-43b84f5cb5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "There have been various investigations and reports looking into potential links between Donald Trump and Russian officials, particularly regarding alleged interference in the 2016 U.S. presidential election. Some of these investigations have looked into possible collusion or connections between Trump's campaign and Russian officials, but the details and conclusions vary depending on the source.\n",
      "[Document(page_content='What is the link between Donald Trump and Russian Officials?', metadata={'source': 'docs\\\\user_query.txt'}), Document(page_content='What is the link between Donald Trump and Russian Officials?', metadata={'source': 'docs\\\\user_query.txt'}), Document(page_content='What is the link between Donald Trump and Russian Officials?', metadata={'source': 'docs\\\\user_query.txt'}), Document(page_content='What is the link between Donald Trump and Russian Officials?', metadata={'source': 'docs\\\\user_query.txt'})]\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "response = qa_ans(question)\n",
    "\n",
    "with open('./docs/response_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(response, f)\n",
    "print(response['result'])\n",
    "print(response['source_documents'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
